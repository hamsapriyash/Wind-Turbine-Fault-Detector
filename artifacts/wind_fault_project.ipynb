{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05283034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (40000, 41)\n",
      "Test shape : (10000, 41)\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -4.464606 -4.679129  3.101546  0.506130 -0.221083 -2.032511 -2.910870   \n",
      "1 -2.909996 -2.568662  4.109032  1.316672 -1.620594 -3.827212 -1.616970   \n",
      "2  4.283674  5.105381  6.092238  2.639922 -1.041357  1.308419 -1.876140   \n",
      "3  3.365912  3.653381  0.909671 -1.367528  0.332016  2.358938  0.732600   \n",
      "4 -3.831843 -5.824444  0.634031 -2.418815 -1.773827  1.016824 -2.098941   \n",
      "\n",
      "         V8        V9       V10  ...       V32       V33       V34       V35  \\\n",
      "0  0.050714 -1.522351  3.761892  ...  3.059700 -1.690440  2.846296  2.235198   \n",
      "1  0.669006  0.387045  0.853814  ... -3.782686 -6.823172  4.908562  0.481554   \n",
      "2 -9.582412  3.469504  0.763395  ... -3.097934  2.690334 -1.643048  7.566482   \n",
      "3 -4.332135  0.565695 -0.101080  ... -1.795474  3.032780 -2.467514  1.894599   \n",
      "4 -3.173204 -2.081860  5.392621  ... -0.257101  0.803550  4.086219  2.292138   \n",
      "\n",
      "        V36       V37       V38       V39       V40  Target  \n",
      "0  6.667486  0.443809 -2.369169  2.950578 -3.480324       0  \n",
      "1  5.338051  2.381297 -3.127756  3.527309 -3.019581       0  \n",
      "2 -3.197647 -3.495672  8.104779  0.562085 -4.227426       0  \n",
      "3 -2.297780 -1.731048  5.908837 -0.386345  0.616242       0  \n",
      "4  5.360850  0.351993  2.940021  3.839160 -4.309402       0  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Wind Turbine Fault Detection — Full Notebook\n",
    "#\n",
    "# This Python script is formatted as notebook cells (\"# %%\" separators). Save as\n",
    "# `wind_fault_full_notebook.py` and open in VSCode/Jupyter (Jupytext will convert\n",
    "# to an .ipynb). It implements everything we ran: load data, binarize target,\n",
    "# handle missing values, balance training set (SMOTE if available, else simple\n",
    "# oversampling), feature selection (Mutual Information + XGBoost if available),\n",
    "# train RandomForest, evaluate, visualize, save artifacts, and (optional) SHAP\n",
    "# explanations.\n",
    "\n",
    "# %%\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# import imblearn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "# import joblib\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Configuration — paths and options\n",
    "\n",
    "# %%\n",
    "# Paths (put Train.csv and Test.csv in the same folder as this script/notebook)\n",
    "DATA_DIR = '.'  # change if files are elsewhere\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'Train.csv')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'Test.csv')\n",
    "OUT_DIR = os.path.join(DATA_DIR, 'artifacts')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Options\n",
    "USE_SMOTE = True        # try to use SMOTE; will fall back if imblearn not installed\n",
    "RANDOM_STATE = 42\n",
    "TOP_K = 15\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Step 1 — Load data and quick inspection\n",
    "\n",
    "# %%\n",
    "# Load\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape :', test.shape)\n",
    "\n",
    "# Quick head\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5496fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Target unique values: [np.int64(0), np.int64(1)]\n",
      "Original binary counts: {0: 37813, 1: 2187}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 2 — Binarize target and examine class imbalance\n",
    "\n",
    "# %%\n",
    "if 'Target' not in train.columns:\n",
    "    raise ValueError(\"Train.csv must contain a 'Target' column.\")\n",
    "\n",
    "# Binarize: non-zero -> 1\n",
    "train['Target_bin'] = (train['Target'] != 0).astype(int)\n",
    "\n",
    "print('Original Target unique values:', sorted(train['Target'].unique()))\n",
    "print('Original binary counts:', train['Target_bin'].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11dc85",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3 — Handle missing values (median imputation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab53542",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in train.columns if c.startswith('V')]\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError('No feature columns found starting with \"V\"')\n",
    "\n",
    "# Fill NaNs with median (train medians)\n",
    "medians = train[feature_cols].median()\n",
    "train[feature_cols] = train[feature_cols].fillna(medians)\n",
    "test[feature_cols] = test[feature_cols].fillna(medians)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69eb867",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 — Train/validation stratified split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fecc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution before resample: Counter({0: 30250, 1: 1750})\n",
      "Val distribution: Counter({0: 7563, 1: 437})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "X = train[feature_cols].copy()\n",
    "y = train['Target_bin'].copy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "print('Train distribution before resample:', Counter(y_train))\n",
    "print('Val distribution:', Counter(y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c76a7",
   "metadata": {},
   "source": [
    "## Step 5 — Balance training set (SMOTE preferred, fallback oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931b625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be506337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE applied. New train distribution: Counter({0: 30250, 1: 30250})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "X_train_res = X_train.copy()\n",
    "y_train_res = y_train.copy()\n",
    "\n",
    "if USE_SMOTE:\n",
    "    try:\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        sm = SMOTE(random_state=RANDOM_STATE)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train_res, y_train_res)\n",
    "        print('SMOTE applied. New train distribution:', Counter(y_train_res))\n",
    "    except Exception as e:\n",
    "        print('SMOTE not available or failed:', e)\n",
    "        # fallback to simple upsampling\n",
    "\n",
    "# simple random upsampling fallback if SMOTE not applied or disabled\n",
    "if Counter(y_train_res)[0] != Counter(y_train_res)[1]:\n",
    "    cnt = Counter(y_train_res)\n",
    "    maj = 0 if cnt[0] > cnt[1] else 1\n",
    "    minc = 1 - maj\n",
    "    idx_maj = y_train_res[y_train_res==maj].index\n",
    "    idx_min = y_train_res[y_train_res==minc].index\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    resample_idx = rng.choice(idx_min, size=(len(idx_maj)-len(idx_min)), replace=True)\n",
    "    X_train_res = pd.concat([X_train_res.loc[idx_maj], X_train_res.loc[idx_min], X_train_res.loc[resample_idx]])\n",
    "    y_train_res = pd.concat([y_train_res.loc[idx_maj], y_train_res.loc[idx_min], y_train_res.loc[resample_idx]])\n",
    "    print('After simple oversampling train counts:', Counter(y_train_res))\n",
    "\n",
    "# shuffle\n",
    "X_train_res = X_train_res.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "y_train_res = y_train_res.loc[X_train_res.index].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5bdf05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features by mutual info:\n",
      " V28    0.004239\n",
      "V38    0.004055\n",
      "V36    0.003871\n",
      "V37    0.003826\n",
      "V8     0.002498\n",
      "V5     0.002213\n",
      "V39    0.001988\n",
      "V3     0.001720\n",
      "V34    0.001261\n",
      "V21    0.001183\n",
      "dtype: float64\n",
      "Top features by XGBoost importance:\n",
      " V40    0.028686\n",
      "V8     0.027415\n",
      "V21    0.027272\n",
      "V24    0.027190\n",
      "V27    0.026182\n",
      "V18    0.026170\n",
      "V26    0.026134\n",
      "V35    0.026121\n",
      "V22    0.026043\n",
      "V32    0.026020\n",
      "dtype: float32\n",
      "Top 15 features selected: ['V8', 'V21', 'V36', 'V37', 'V27', 'V38', 'V35', 'V18', 'V22', 'V30', 'V5', 'V40', 'V34', 'V16', 'V24']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Step 6 — Feature selection (Mutual Information + XGBoost if available)\n",
    "\n",
    "# %%\n",
    "# Mutual information (nonparametric, works for continuous features)\n",
    "mi = mutual_info_classif(X_train_res, y_train_res, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi, index=feature_cols).sort_values(ascending=False)\n",
    "print('Top features by mutual info:\\n', mi_series.head(10))\n",
    "\n",
    "# Try XGBoost feature importances\n",
    "xgb_available = True\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE, verbosity=0)\n",
    "    xgb_clf.fit(X_train_res, y_train_res)\n",
    "    xgb_imp = pd.Series(xgb_clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "    print('Top features by XGBoost importance:\\n', xgb_imp.head(10))\n",
    "except Exception as e:\n",
    "    xgb_available = False\n",
    "    xgb_imp = pd.Series(0, index=feature_cols)\n",
    "    print('XGBoost unavailable or failed to run:', e)\n",
    "\n",
    "# Combine ranks (average of MI rank and XGB rank) to get robust ranking\n",
    "mi_rank = mi_series.rank(ascending=False)\n",
    "xgb_rank = xgb_imp.rank(ascending=False)\n",
    "combined_rank = ((mi_rank + xgb_rank) / 2.0).sort_values()\n",
    "\n",
    "top_features = combined_rank.index[:TOP_K].tolist()\n",
    "print(f'Top {TOP_K} features selected:', top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcdaa6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.52      0.67      7563\n",
      "           1       0.05      0.45      0.09       437\n",
      "\n",
      "    accuracy                           0.52      8000\n",
      "   macro avg       0.50      0.48      0.38      8000\n",
      "weighted avg       0.89      0.52      0.64      8000\n",
      "\n",
      "Confusion matrix:\n",
      " [[3939 3624]\n",
      " [ 242  195]]\n",
      "PR-AUC: 0.048632259642445094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Step 7 — Train model (RandomForest) on top features\n",
    "\n",
    "# %%\n",
    "# Train RandomForest as a robust baseline\n",
    "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "rf.fit(X_train_res[top_features], y_train_res)\n",
    "\n",
    "# Validation\n",
    "y_val_pred = rf.predict(X_val[top_features])\n",
    "if hasattr(rf, 'predict_proba'):\n",
    "    y_val_prob = rf.predict_proba(X_val[top_features])[:,1]\n",
    "else:\n",
    "    y_val_prob = None\n",
    "\n",
    "print('Validation classification report:')\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "# PR-AUC\n",
    "if y_val_prob is not None:\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_val_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print('PR-AUC:', pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c90eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test predictions to .\\artifacts\\test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### Step 8 — Save model, scaler (optional) and test predictions\n",
    "\n",
    "# %%\n",
    "# Save model\n",
    "# joblib.dump(rf, os.path.join(OUT_DIR, 'final_model.joblib'))\n",
    "\n",
    "# Predict on test set (if test has no Target)\n",
    "if set(feature_cols).issubset(set(test.columns)):\n",
    "    test_preds = rf.predict(test[top_features])\n",
    "    test_probs = rf.predict_proba(test[top_features])[:,1] if hasattr(rf, 'predict_proba') else None\n",
    "    test_out = test.copy()\n",
    "    test_out['pred'] = test_preds\n",
    "    if test_probs is not None:\n",
    "        test_out['pred_prob'] = test_probs\n",
    "    test_out.to_csv(os.path.join(OUT_DIR, 'test_predictions.csv'), index=False)\n",
    "    print('Saved test predictions to', os.path.join(OUT_DIR, 'test_predictions.csv'))\n",
    "else:\n",
    "    print('Test file does not contain expected feature columns; skipping test predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f749070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 10 — (Optional) SHAP explanations per prediction (local attribution)\n",
    "#\n",
    "# SHAP is the recommended way to show, for each faulty prediction, which features\n",
    "# pushed the model toward the fault prediction. This block runs only if `shap`\n",
    "# is installed. You can skip if not available.\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    import shap\n",
    "    # Use TreeExplainer for tree models\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    # compute on validation set (top features)\n",
    "    shap_values = explainer.shap_values(X_val[top_features])\n",
    "    # Example: get top 3 positive contributors for the first faulty prediction\n",
    "    idxs_faulty = np.where((rf.predict(X_val[top_features]) == 1) & (y_val == 1))[0]\n",
    "    if len(idxs_faulty) > 0:\n",
    "        i = idxs_faulty[0]\n",
    "        sv = shap_values[1][i] if isinstance(shap_values, list) else shap_values[i]\n",
    "        contribs = pd.Series(sv, index=top_features).sort_values(ascending=False)\n",
    "        print('Top 3 contributing features for one faulty instance:')\n",
    "        print(contribs.head(3))\n",
    "        # Save a SHAP force plot HTML\n",
    "        shap_html = os.path.join(OUT_DIR, 'shap_force_example.html')\n",
    "        shap.force_plot(explainer.expected_value[1], sv, X_val[top_features].iloc[i], matplotlib=False, show=False, out_names=None)\n",
    "        # Note: shap.force_plot HTML saving requires shap.plots._force.manual\n",
    "    else:\n",
    "        print('No faulty instances in validation predictions to demo SHAP')\n",
    "except Exception as e:\n",
    "    print('shap not available or failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399072d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Step 9 — Visualizations (save PNGs)\n",
    "\n",
    "# %%\n",
    "# Class distributions\n",
    "plt.figure(figsize=(6,4))\n",
    "orig_counts = train['Target_bin'].value_counts()\n",
    "plt.bar(['0 (normal)', '1 (fault)'], [orig_counts.get(0,0), orig_counts.get(1,0)])\n",
    "plt.title('Original class distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'class_dist_original.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "train_counts_after = Counter(y_train_res)\n",
    "plt.bar(['0 (normal)', '1 (fault)'], [train_counts_after.get(0,0), train_counts_after.get(1,0)])\n",
    "plt.title('Training distribution after balancing')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'class_dist_after_balancing.png'))\n",
    "plt.close()\n",
    "\n",
    "# Top features bar chart (combined rank values)\n",
    "plt.figure(figsize=(8,6))\n",
    "scores = combined_rank.loc[top_features].sort_values(ascending=True)\n",
    "plt.barh(scores.index, scores.values)\n",
    "plt.title('Top features (combined rank)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'top_features_combined_rank.png'))\n",
    "plt.close()\n",
    "\n",
    "# PCA on top features\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(pd.concat([X_train_res[top_features], X_val[top_features]]))\n",
    "y_pca = pd.concat([y_train_res, y_val])\n",
    "plt.figure(figsize=(7,6))\n",
    "for cls in sorted(y_pca.unique()):\n",
    "    mask = (y_pca == cls).values\n",
    "    plt.scatter(X_pca[mask,0], X_pca[mask,1], label=str(cls), s=10, alpha=0.6)\n",
    "plt.legend(title='Class')\n",
    "plt.title('PCA (2D) of Top Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'pca_top_features.png'))\n",
    "plt.close()\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='viridis')\n",
    "plt.title('Confusion Matrix (Validation)')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.xticks([0,1])\n",
    "plt.yticks([0,1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'confusion_matrix.png'))\n",
    "plt.close()\n",
    "\n",
    "print('Saved visualizations to', OUT_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### End — summary of saved artifacts\n",
    "\n",
    "# %%\n",
    "print('\\nArtifacts saved in', OUT_DIR)\n",
    "for fn in os.listdir(OUT_DIR):\n",
    "    print('-', fn)\n",
    "\n",
    "print('\\nTop features list saved in variable top_features')\n",
    "print('To run this notebook interactively: open this file in Jupyter/VSCode or convert to .ipynb with jupytext.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
